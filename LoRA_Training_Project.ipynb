{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **GÃ¶rev 1: Google Colab'Ä± HazÄ±rla**"
      ],
      "metadata": {
        "id": "YX_T_H9117eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*AdÄ±m 1: Gerekli KÃ¼tÃ¼phaneleri YÃ¼kle*"
      ],
      "metadata": {
        "id": "BHgdDx8H1tSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proje iÃ§in gerekli kÃ¼tÃ¼phaneleri yÃ¼klÃ¼yoruz\n",
        "# qwen modeli, lora (peft), ve veri iÅŸleme araÃ§larÄ±\n",
        "# Flash Attention'Ä± Ã§Ä±kardÄ±k, T4 GPU ile uyumlu olanlarÄ± yÃ¼klÃ¼yoruz\n",
        "# Bu kodun hatasÄ±z Ã§alÄ±ÅŸmasÄ± lazÄ±m\n",
        "!pip install torch transformers datasets peft bitsandbytes accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w4HRelz0z-u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*AdÄ±m 2: Hugging Face'e GiriÅŸ Yap*"
      ],
      "metadata": {
        "id": "7uqJT87A2DJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hugging Face'e GiriÅŸ Yap\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Ã‡alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nda bir kutucuk Ã§Ä±kacak.\n",
        "# Token'Ä± oraya yapÄ±ÅŸtÄ±r ve Enter'a bas.\n",
        "login()"
      ],
      "metadata": {
        "id": "S5WCUXfq0RSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*AdÄ±m 3: Base Modeli Ä°ndirip Test Et*"
      ],
      "metadata": {
        "id": "xGZ3tZ2D2JBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modeli Ä°ndir ve Test Et (GÃ¶rev 1)\n",
        "#DokÃ¼manÄ±n istediÄŸi Qwen2.5-Coder-1.5B-Instruct modelini indireceÄŸiz ve basit bir soru soracaÄŸÄ±z.\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1. DokÃ¼manÄ±n istediÄŸi zorunlu modeli seÃ§iyoruz\n",
        "model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "print(\"Model indiriliyor... (Bu iÅŸlem 1-2 dakika sÃ¼rebilir)\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16, # HafÄ±za dostu mod\n",
        "    device_map=\"auto\" # GPU'yu otomatik kullan\n",
        ")\n",
        "\n",
        "# 2. Test sorusu soralÄ±m\n",
        "soru = \"Write a python function to check if a number is prime.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": soru}\n",
        "]\n",
        "\n",
        "# 3. CevabÄ± Ã¼rettirelim\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- MODELÄ°N CEVABI ---\")\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zys0VErq0aCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GÃ¶rev 2: Dataset Analizi**"
      ],
      "metadata": {
        "id": "OBIpnOXH1gqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*AdÄ±m 1: Verileri Ä°ndir*"
      ],
      "metadata": {
        "id": "Ze7fYp2929MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"1. DEEP Dataset indiriliyor...\")\n",
        "# DokÃ¼mandaki zorunlu DEEP dataset'i indiriyoruz\n",
        "dataset_deep = load_dataset(\"Naholav/CodeGen-Deep-5K\", split=\"train\")\n",
        "\n",
        "print(\"2. DIVERSE Dataset indiriliyor...\")\n",
        "# DokÃ¼mandaki zorunlu DIVERSE dataset'i indiriyoruz\n",
        "dataset_diverse = load_dataset(\"Naholav/CodeGen-Diverse-5K\", split=\"train\")\n",
        "\n",
        "print(\"\\n--- Ä°NDÄ°RME TAMAMLANDI ---\")\n",
        "print(f\"DEEP Dataset Soru SayÄ±sÄ±: {len(dataset_deep)}\")\n",
        "print(f\"DIVERSE Dataset Soru SayÄ±sÄ±: {len(dataset_diverse)}\")\n",
        "\n",
        "# Bir Ã¶rnek inceleyelim\n",
        "print(\"\\n--- DEEP DATASET'TEN BÄ°R Ã–RNEK ---\")\n",
        "example = dataset_deep[0]\n",
        "print(f\"SORU (Input): {example['input'][:100]}...\") # Sorunun baÅŸÄ±\n",
        "print(f\"\\nÃ‡Ã–ZÃœM (Solution): {example['solution'][:100]}...\") # CevabÄ±n baÅŸÄ±\n",
        "\n",
        "print(\"\\n--- KRÄ°TÄ°K BÄ°LGÄ° ---\")\n",
        "print(\"DokÃ¼mana gÃ¶re eÄŸitimde sadece 'solution' alanÄ±nÄ± kullanacaÄŸÄ±z.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WypHr7fA3EEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GÃ¶rev 3: Training (EÄŸitim)**"
      ],
      "metadata": {
        "id": "I19_vFSv3X8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EÄŸitim kodunu Ã§ok daha kolay yazmak iÃ§in trl adÄ±nda popÃ¼ler bir kÃ¼tÃ¼phane kullanacaÄŸÄ±z.\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nuFiAIa63o9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***AdÄ±m 1: EÄŸitimi BaÅŸlatÄ±yoruz (DEEP Dataset)***\n",
        "\n",
        "AÅŸaÄŸÄ±daki kod bloÄŸu:\n",
        "\n",
        "-Modeli eÄŸitime hazÄ±rlÄ±yor (LoRA ayarlarÄ±nÄ± yapÄ±yor) .\n",
        "\n",
        "-Veriyi modelin anlayacaÄŸÄ± formata Ã§eviriyor (System Prompt ekliyor).\n",
        "\n",
        "-Bu kod DEEP Dataset  kullanarak eÄŸitimi baÅŸlatacak ve her ÅŸeyi /content/drive/MyDrive/qwen-deep-lora-model klasÃ¶rÃ¼ne kaydedecek."
      ],
      "metadata": {
        "id": "ypq-YkbU34RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Google Drive BaÄŸlantÄ±sÄ± (Sigorta!)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. KÃ¼tÃ¼phaneleri Kur (Sabit Versiyon)\n",
        "!pip install torch transformers datasets peft bitsandbytes accelerate\n",
        "!pip install trl==0.8.6\n",
        "\n",
        "# 3. Hugging Face GiriÅŸi\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "# Token'Ä±nÄ± gir ve Enter'a bas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5XWmlKhFis1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- 1. AYARLAR ---\n",
        "model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\" # [cite: 24]\n",
        "dataset_name = \"DEEP_Dataset\"\n",
        "\n",
        "# --- KAYIT YERÄ°: GOOGLE DRIVE ---\n",
        "# Bilgisayar kapansa bile silinmez!\n",
        "output_dir = \"/content/drive/MyDrive/qwen-deep-lora-model\"\n",
        "\n",
        "# --- 2. MODEL VE TOKENIZER ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# --- 3. LORA AYARLARI ---\n",
        "# DokÃ¼mana uygun rank ve modÃ¼ller [cite: 91, 94]\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- 4. VERÄ° FORMATLAMA ---\n",
        "# Zorunlu system prompt [cite: 131]\n",
        "system_prompt = \"You are an expert Python programmer. Please read the problem carefully before writing any Python code.\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    output_texts = []\n",
        "    for i in range(len(examples['input'])):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": examples['input'][i]},\n",
        "            {\"role\": \"assistant\", \"content\": examples['solution'][i]} # Sadece solution [cite: 44]\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# --- DATASET SEÃ‡Ä°MÄ°: DEEP ---\n",
        "print(\"DEEP Dataset Ä°ndiriliyor...\")\n",
        "dataset = load_dataset(\"Naholav/CodeGen-Deep-5K\", split=\"train\") # [cite: 10]\n",
        "\n",
        "# --- 5. EÄÄ°TÄ°M AYARLARI (HafÄ±za Dostu & Drive KayÄ±tlÄ±) ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,      # HafÄ±za iÃ§in 1 [cite: 181]\n",
        "    gradient_accumulation_steps=16,     # Dengelemek iÃ§in 16 [cite: 112]\n",
        "    gradient_checkpointing=True,        # OOM Ã¶nleyici [cite: 119]\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# --- 6. BAÅLAT ---\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length=1024, # Sadece kod iÃ§in 1024 yeterli [cite: 108]\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "print(\"--- EÄÄ°TÄ°M BAÅLIYOR (DEEP DATASET) -> DRIVE'A KAYDEDÄ°LÄ°YOR ---\")\n",
        "trainer.train()\n",
        "\n",
        "# --- 7. SON KAYIT ---\n",
        "print(\"EÄŸitim bitti, son kontroller yapÄ±lÄ±yor...\")\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "print(f\"TEBRÄ°KLER! Modeliniz ÅŸurada gÃ¼venle saklandÄ±: {output_dir}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SJANbE8DjAAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***AdÄ±m 2: EÄŸitimi BaÅŸlatÄ±yoruz (DIVERSE Dataset)***\n",
        "\n",
        "AÅŸaÄŸÄ±daki kod bloÄŸu:\n",
        "\n",
        "-Bu kod DIVERSE Dataset  kullanarak eÄŸitimi baÅŸlatacak ve her ÅŸeyi /content/drive/MyDrive/qwen-deep-lora-model klasÃ¶rÃ¼ne kaydedecek."
      ],
      "metadata": {
        "id": "cr1H9oXo0T3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. KURULUMLAR VE BAÄLANTILAR ---\n",
        "print(\"Ortam hazÄ±rlanÄ±yor...\")\n",
        "!pip install -q torch transformers datasets peft bitsandbytes accelerate\n",
        "!pip install -q trl==0.8.6\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "login() # Token'Ä±nÄ± girmen gerekecek\n"
      ],
      "metadata": {
        "id": "O2p5wOcH8l_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. EÄÄ°TÄ°M KODU (DIVERSE DATASET) ---\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Ayarlar\n",
        "model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "dataset_name = \"DIVERSE_Dataset\" # [cite: 12]\n",
        "output_dir = \"/content/drive/MyDrive/qwen-diverse-lora-model\" # Drive'a yeni klasÃ¶r\n",
        "\n",
        "# Model ve Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# LoRA AyarlarÄ±\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Veri Formatlama\n",
        "system_prompt = \"You are an expert Python programmer. Please read the problem carefully before writing any Python code.\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    output_texts = []\n",
        "    for i in range(len(examples['input'])):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": examples['input'][i]},\n",
        "            {\"role\": \"assistant\", \"content\": examples['solution'][i]}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# DIVERSE Dataset Ä°ndiriliyor [cite: 12]\n",
        "print(\"DIVERSE Dataset Ä°ndiriliyor...\")\n",
        "dataset = load_dataset(\"Naholav/CodeGen-Diverse-5K\", split=\"train\")\n",
        "\n",
        "# EÄŸitim AyarlarÄ± (HafÄ±za Dostu)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# BaÅŸlat\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length=1024,\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "print(\"--- EÄÄ°TÄ°M BAÅLIYOR (DIVERSE) -> DRIVE'A KAYDEDÄ°LÄ°YOR ---\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"EÄŸitim bitti, kaydediliyor...\")\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "print(f\"HARÄ°KA! DIVERSE modeli de ÅŸuraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "Uoepr0Ee8rFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# 1. Ã–nce GiriÅŸ YapalÄ±m (Emin olmak iÃ§in)\n",
        "print(\"Hugging Face'e giriÅŸ yapÄ±lÄ±yor...\")\n",
        "login()\n",
        "\n",
        "# --- AYARLAR (BURAYI DÃœZENLE!) ---\n",
        "# LÃ¼tfen tÄ±rnak iÃ§ine kendi Hugging Face kullanÄ±cÄ± adÄ±nÄ± yaz\n",
        "username = \"caglademir\"\n",
        "# Ã–rnek: username = \"ahmetyilmaz\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# YÃ¼kleme Fonksiyonu\n",
        "def modele_yukle(drive_path, model_name):\n",
        "    repo_id = f\"{username}/{model_name}\"\n",
        "    print(f\"\\nğŸš€ BaÅŸlatÄ±lÄ±yor: {repo_id}\")\n",
        "    print(f\"Kaynak: {drive_path}\")\n",
        "\n",
        "    try:\n",
        "        # Repoyu oluÅŸtur (varsa hata verme)\n",
        "        api.create_repo(repo_id=repo_id, exist_ok=True, repo_type=\"model\")\n",
        "\n",
        "        # DosyalarÄ± yÃ¼kle\n",
        "        api.upload_folder(\n",
        "            folder_path=drive_path,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        print(f\"âœ… BAÅARILI! Modelin ÅŸurada yayÄ±nda:\")\n",
        "        print(f\"ğŸ‘‰ https://huggingface.co/{repo_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- Ä°ÅLEM BAÅLIYOR ---\n",
        "\n",
        "# 1. DEEP Modeli YÃ¼kle\n",
        "modele_yukle(\n",
        "    drive_path=\"/content/drive/MyDrive/qwen-deep-lora-model\",\n",
        "    model_name=\"Qwen2.5-Coder-1.5B-LoRA-DEEP\"\n",
        ")\n",
        "\n",
        "# 2. DIVERSE Modeli YÃ¼kle\n",
        "modele_yukle(\n",
        "    drive_path=\"/content/drive/MyDrive/qwen-diverse-lora-model\",\n",
        "    model_name=\"Qwen2.5-Coder-1.5B-LoRA-DIVERSE\"\n",
        ")"
      ],
      "metadata": {
        "id": "JnsxqlnePsZe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}