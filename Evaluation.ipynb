{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46AmNfksWduV"
      },
      "outputs": [],
      "source": [
        "# --- EVALUATION SCRIPT (DEĞERLENDİRME KODU) ---\n",
        "# Bu kod, eğitilen modeli test verisi üzerinde dener.\n",
        "\n",
        "# 1. Gerekli Kütüphaneler\n",
        "!pip install -q torch transformers peft datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 2. AYARLAR\n",
        "# Base model (Orjinal hali)\n",
        "base_model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "# Senin eğittiğin model (Hugging Face'deki ismi)\n",
        "# BURAYI DÜZENLE: Kendi kullanıcı adını yaz\n",
        "adapter_model_id = \"caglademir/Qwen2.5-Coder-1.5B-LoRA-DEEP\"\n",
        "\n",
        "print(f\"Test edilecek model: {adapter_model_id}\")\n",
        "\n",
        "# 3. MODELİ YÜKLE\n",
        "# Önce base modeli yüklüyoruz\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Sonra senin eğittiğin LoRA adaptörünü üzerine ekliyoruz\n",
        "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# Modeli değerlendirme moduna al\n",
        "model.eval()\n",
        "\n",
        "# 4. TEST VERİSİNİ HAZIRLA\n",
        "print(\"Test verisi yükleniyor...\")\n",
        "# Dokümana göre test split kullanılmalı\n",
        "try:\n",
        "    dataset = load_dataset(\"Naholav/CodeGen-Deep-5K\", split=\"test\")\n",
        "except:\n",
        "    # Eğer test split yoksa train'den küçük bir parça al\n",
        "    print(\"Test split bulunamadı, Train setinden örnek alınıyor.\")\n",
        "    dataset = load_dataset(\"Naholav/CodeGen-Deep-5K\", split=\"train\").select(range(5))\n",
        "\n",
        "# 5. ÖRNEK SORU ÇÖZME FONKSİYONU\n",
        "def evaluate_model(index=0):\n",
        "    input_text = dataset[index]['input']\n",
        "    ground_truth = dataset[index]['solution']\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(f\"\\n--- SORU {index+1} ---\")\n",
        "    print(input_text[:200] + \"...\")\n",
        "\n",
        "    # Model cevap üretiyor\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs.input_ids, max_new_tokens=256)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Cevabı temizle (Sadece üretilen kısmı al)\n",
        "    response_code = response.split(prompt)[-1] if prompt in response else response\n",
        "\n",
        "    print(\"\\n--- MODELİN CEVABI ---\")\n",
        "    print(response_code)\n",
        "    print(\"\\n--- GERÇEK CEVAP (SOLUTION) ---\")\n",
        "    print(ground_truth[:300] + \"...\")\n",
        "\n",
        "# 6. TESTİ ÇALIŞTIR\n",
        "print(\"Değerlendirme Başlıyor...\")\n",
        "evaluate_model(0)\n",
        "evaluate_model(1)"
      ]
    }
  ]
}